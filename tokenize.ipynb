{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f437ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['เมื่อ', 'วัน', 'พฤหัสบดี', 'ที่', ' ', '31', ' ', 'ก.ค.', ' ', '2568', ' ', 'โดนัลด์', ' ', 'ทรัมป์', ' ', 'ประธานาธิบดี', 'สหรัฐฯ', ' ', 'ประกาศ', 'ผ่าน', ' ', 'Truth', ' ', 'Social', ' ', 'ว่า', ' ', 'เขา', 'จะ', 'เลื่อน', 'การบังคับใช้', 'มาตรการ', 'ภาษี', 'ต่าง', 'ตอบแทน', 'อัตรา', ' ', '30', '%', ' ', 'กับ', 'เม็กซิโก', 'ไป', 'ก่อน', ' ', 'โดย', 'จะ', 'ใช้', 'ข้อตกลง', 'เดิม', 'ซึ่ง', 'จะ', 'เก็บภาษี', 'สินค้า', 'บาง', 'รายการ', 'ที่', 'นำเข้า', 'จาก', 'เม็กซิโก', 'ใน', 'อัตรา', ' ', '25', '%', ' ', 'ต่อไป', 'อีก', ' ', '90', ' ', 'วัน', ' ', 'ในระหว่างนั้น', 'ทั้งสองฝ่าย', 'จะ', 'หาทาง', 'ทำ', 'ข้อตกลง', 'การค้า', 'ร่วมกัน']\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "from pythainlp.tokenize import word_tokenize\n",
    "\n",
    "text = \"เมื่อวันพฤหัสบดีที่ 31 ก.ค. 2568 โดนัลด์ ทรัมป์ ประธานาธิบดีสหรัฐฯ ประกาศผ่าน Truth Social ว่า เขาจะเลื่อนการบังคับใช้มาตรการภาษีต่างตอบแทนอัตรา 30% กับเม็กซิโกไปก่อน โดยจะใช้ข้อตกลงเดิมซึ่งจะเก็บภาษีสินค้าบางรายการที่นำเข้าจากเม็กซิโกในอัตรา 25% ต่อไปอีก 90 วัน ในระหว่างนั้นทั้งสองฝ่ายจะหาทางทำข้อตกลงการค้าร่วมกัน\"\n",
    "all_words = word_tokenize(text, engine=\"newmm\") \n",
    "print(all_words)\n",
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a0ee0ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'เมื่อ': 0,\n",
       " 'วัน': 71,\n",
       " 'พฤหัสบดี': 2,\n",
       " 'ที่': 56,\n",
       " ' ': 72,\n",
       " '31': 5,\n",
       " 'ก.ค.': 7,\n",
       " '2568': 9,\n",
       " 'โดนัลด์': 11,\n",
       " 'ทรัมป์': 13,\n",
       " 'ประธานาธิบดี': 15,\n",
       " 'สหรัฐฯ': 16,\n",
       " 'ประกาศ': 18,\n",
       " 'ผ่าน': 19,\n",
       " 'Truth': 21,\n",
       " 'Social': 23,\n",
       " 'ว่า': 25,\n",
       " 'เขา': 27,\n",
       " 'จะ': 75,\n",
       " 'เลื่อน': 29,\n",
       " 'การบังคับใช้': 30,\n",
       " 'มาตรการ': 31,\n",
       " 'ภาษี': 32,\n",
       " 'ต่าง': 33,\n",
       " 'ตอบแทน': 34,\n",
       " 'อัตรา': 61,\n",
       " '30': 37,\n",
       " '%': 64,\n",
       " 'กับ': 40,\n",
       " 'เม็กซิโก': 59,\n",
       " 'ไป': 42,\n",
       " 'ก่อน': 43,\n",
       " 'โดย': 45,\n",
       " 'ใช้': 47,\n",
       " 'ข้อตกลง': 78,\n",
       " 'เดิม': 49,\n",
       " 'ซึ่ง': 50,\n",
       " 'เก็บภาษี': 52,\n",
       " 'สินค้า': 53,\n",
       " 'บาง': 54,\n",
       " 'รายการ': 55,\n",
       " 'นำเข้า': 57,\n",
       " 'จาก': 58,\n",
       " 'ใน': 60,\n",
       " '25': 63,\n",
       " 'ต่อไป': 66,\n",
       " 'อีก': 67,\n",
       " '90': 69,\n",
       " 'ในระหว่างนั้น': 73,\n",
       " 'ทั้งสองฝ่าย': 74,\n",
       " 'หาทาง': 76,\n",
       " 'ทำ': 77,\n",
       " 'การค้า': 79,\n",
       " 'ร่วมกัน': 80}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddf57fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('เมื่อ', 0)\n",
      "('วัน', 71)\n",
      "('พฤหัสบดี', 2)\n",
      "('ที่', 56)\n",
      "(' ', 72)\n",
      "('31', 5)\n",
      "('ก.ค.', 7)\n",
      "('2568', 9)\n",
      "('โดนัลด์', 11)\n",
      "('ทรัมป์', 13)\n",
      "('ประธานาธิบดี', 15)\n",
      "('สหรัฐฯ', 16)\n",
      "('ประกาศ', 18)\n",
      "('ผ่าน', 19)\n",
      "('Truth', 21)\n",
      "('Social', 23)\n",
      "('ว่า', 25)\n",
      "('เขา', 27)\n",
      "('จะ', 75)\n",
      "('เลื่อน', 29)\n",
      "('การบังคับใช้', 30)\n",
      "('มาตรการ', 31)\n",
      "('ภาษี', 32)\n",
      "('ต่าง', 33)\n",
      "('ตอบแทน', 34)\n",
      "('อัตรา', 61)\n",
      "('30', 37)\n",
      "('%', 64)\n",
      "('กับ', 40)\n",
      "('เม็กซิโก', 59)\n",
      "('ไป', 42)\n",
      "('ก่อน', 43)\n",
      "('โดย', 45)\n",
      "('ใช้', 47)\n",
      "('ข้อตกลง', 78)\n",
      "('เดิม', 49)\n",
      "('ซึ่ง', 50)\n",
      "('เก็บภาษี', 52)\n",
      "('สินค้า', 53)\n",
      "('บาง', 54)\n",
      "('รายการ', 55)\n",
      "('นำเข้า', 57)\n",
      "('จาก', 58)\n",
      "('ใน', 60)\n",
      "('25', 63)\n",
      "('ต่อไป', 66)\n",
      "('อีก', 67)\n",
      "('90', 69)\n",
      "('ในระหว่างนั้น', 73)\n",
      "('ทั้งสองฝ่าย', 74)\n",
      "('หาทาง', 76)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9309bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = word_tokenize(text, engine=\"newmm\") \n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68c539da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 75, 29, 30, 31, 32, 33, 34, 61]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"เขาจะเลื่อนการบังคับใช้มาตรการภาษีต่างตอบแทนอัตรา\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98cf03e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'เขา จะ เลื่อน การบังคับใช้ มาตรการ ภาษี ต่าง ตอบแทน อัตรา'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c748335",
   "metadata": {},
   "source": [
    "Adding special context tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf2c6b41",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'--'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m tokenizer = SimpleTokenizerV1(vocab)\n\u001b[32m      3\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mเขาจะเลื่อนการบังคับใช้มาตรการ-- ภาษีต่างตอบแทนอัตรา\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m tokenizer.encode(text)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mSimpleTokenizerV1.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[32m      8\u001b[39m     preprocessed = word_tokenize(text, engine=\u001b[33m\"\u001b[39m\u001b[33mnewmm\u001b[39m\u001b[33m\"\u001b[39m) \n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     ids = [\u001b[38;5;28mself\u001b[39m.str_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[31mKeyError\u001b[39m: '--'"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"เขาจะเลื่อนการบังคับใช้มาตรการ-- ภาษีต่างตอบแทนอัตรา\"\n",
    "\n",
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dec10f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['เมื่อ', 'วัน', 'พฤหัสบดี', 'ที่', ' ']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "571b3db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(all_words)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {s:i for i,s in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a64373f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '%': 1,\n",
       " '25': 2,\n",
       " '2568': 3,\n",
       " '30': 4,\n",
       " '31': 5,\n",
       " '90': 6,\n",
       " 'Social': 7,\n",
       " 'Truth': 8,\n",
       " 'ก.ค.': 9,\n",
       " 'กับ': 10,\n",
       " 'การค้า': 11,\n",
       " 'การบังคับใช้': 12,\n",
       " 'ก่อน': 13,\n",
       " 'ข้อตกลง': 14,\n",
       " 'จะ': 15,\n",
       " 'จาก': 16,\n",
       " 'ซึ่ง': 17,\n",
       " 'ตอบแทน': 18,\n",
       " 'ต่อไป': 19,\n",
       " 'ต่าง': 20,\n",
       " 'ทรัมป์': 21,\n",
       " 'ทั้งสองฝ่าย': 22,\n",
       " 'ทำ': 23,\n",
       " 'ที่': 24,\n",
       " 'นำเข้า': 25,\n",
       " 'บาง': 26,\n",
       " 'ประกาศ': 27,\n",
       " 'ประธานาธิบดี': 28,\n",
       " 'ผ่าน': 29,\n",
       " 'พฤหัสบดี': 30,\n",
       " 'ภาษี': 31,\n",
       " 'มาตรการ': 32,\n",
       " 'รายการ': 33,\n",
       " 'ร่วมกัน': 34,\n",
       " 'วัน': 35,\n",
       " 'ว่า': 36,\n",
       " 'สหรัฐฯ': 37,\n",
       " 'สินค้า': 38,\n",
       " 'หาทาง': 39,\n",
       " 'อัตรา': 40,\n",
       " 'อีก': 41,\n",
       " 'เก็บภาษี': 42,\n",
       " 'เขา': 43,\n",
       " 'เดิม': 44,\n",
       " 'เมื่อ': 45,\n",
       " 'เม็กซิโก': 46,\n",
       " 'เลื่อน': 47,\n",
       " 'โดนัลด์': 48,\n",
       " 'โดย': 49,\n",
       " 'ใช้': 50,\n",
       " 'ใน': 51,\n",
       " 'ในระหว่างนั้น': 52,\n",
       " 'ไป': 53,\n",
       " '<|endoftext|>': 54,\n",
       " '<|unk|>': 55}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "518cf839",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = word_tokenize(text, engine=\"newmm\") \n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e81c712d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "มากเขาจะเลื่อนการบังคับใช้มาตรการภาษี <|endoftext|> ต่างตอบแทนอัตรา\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"มากเขาจะเลื่อนการบังคับใช้มาตรการภาษี\"\n",
    "text2 = \"ต่างตอบแทนอัตรา\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0134982e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[55, 43, 15, 47, 12, 32, 31, 54, 20, 18, 40]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ab74fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|> เขา จะ เลื่อน การบังคับใช้ มาตรการ ภาษี <|endoftext|> ต่าง ตอบแทน อัตรา'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc02b99",
   "metadata": {},
   "source": [
    "Data sampling with a sliding window\n",
    "We train LLMs to generate one word at a time, so we want to prepare the training data accordingly where the next word in a sequence represents the target to predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb7c90e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "543\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1dcc37b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0c4345c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [55, 55, 55, 55]\n",
      "y:      [55, 55, 55, 45]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e25e3ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<|unk|> <|unk|> <|unk|> <|unk|>', '<|unk|> <|unk|> <|unk|> เมื่อ')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(x), tokenizer.decode(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8e3fb8",
   "metadata": {},
   "source": [
    "One by one, the prediction would look like as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be11ae7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|> ----> <|unk|>\n",
      "<|unk|> <|unk|> ----> <|unk|>\n",
      "<|unk|> <|unk|> <|unk|> ----> <|unk|>\n",
      "<|unk|> <|unk|> <|unk|> <|unk|> ----> เมื่อ\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ecd8bfb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f508170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader that extract chunks from the input text dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "        assert len(token_ids) > max_length, \"Number of tokenized inputs must at least be equal to max_length+1\"\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "871c7ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(vocab, txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    print(dataset.input_ids[:5])  # Print first 5 input ids for debugging\n",
    "    print(dataset.target_ids[:5])  # Print first 5 target ids for debugging\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1726ebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2997b18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([55, 55, 55, 24]), tensor([55, 55, 24, 55]), tensor([55, 24, 55, 53]), tensor([24, 55, 53, 55]), tensor([55, 53, 55, 55])]\n",
      "[tensor([55, 55, 24, 55]), tensor([55, 24, 55, 53]), tensor([24, 55, 53, 55]), tensor([55, 53, 55, 55]), tensor([53, 55, 55, 55])]\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    vocab, raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "44a51690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x122f22510>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "49b8b99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[55, 55, 55, 24]]), tensor([[55, 55, 24, 55]])]\n"
     ]
    }
   ],
   "source": [
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "76a96cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[55, 55, 24, 55]]), tensor([[55, 24, 55, 53]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1218f926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[55, 55, 55, 24],\n",
      "        [55, 53, 55, 55],\n",
      "        [55, 55, 36, 55],\n",
      "        [55, 55, 55, 55],\n",
      "        [55, 55, 55, 55],\n",
      "        [55, 55, 55, 55],\n",
      "        [55, 55, 55, 55],\n",
      "        [55, 51, 55, 55]])\n",
      "\n",
      "Targets:\n",
      " tensor([[55, 55, 24, 55],\n",
      "        [53, 55, 55, 55],\n",
      "        [55, 36, 55, 55],\n",
      "        [55, 55, 55, 55],\n",
      "        [55, 55, 55, 55],\n",
      "        [55, 55, 55, 55],\n",
      "        [55, 55, 55, 55],\n",
      "        [51, 55, 55, 55]])\n"
     ]
    }
   ],
   "source": [
    "# We can also create batched outputs\n",
    "# Note that we increase the stride here so that we don't have overlaps between the batches, since more overlap could lead to increased overfitting\n",
    "dataloader = create_dataloader_v1(vocab, raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2973f3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating token embeddings\n",
    "# The data is already almost ready for an LLM\n",
    "# But lastly let us embed the tokens in a continuous vector representation using an embedding layer\n",
    "# Usually, these embedding layers are part of the LLM itself and are updated (trained) during model training\n",
    "# Suppose we have the following four input examples with input ids 2, 3, 5, and 1 (after tokenization):\n",
    "input_ids = torch.tensor([2, 3, 5, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fdbefd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the sake of simplicity, suppose we have a small vocabulary of only 6 words and we want to create embeddings of size 3:\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7becfa06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# This would result in a 6x3 weight matrix:\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "39ce2d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# To convert a token with id 3 into a 3-dimensional vector, we do the following:\n",
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "486ceac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Note that the above is the 4th row in the embedding_layer weight matrix\n",
    "# To embed all four input_ids values above, we do\n",
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b3961096",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 76\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1ac35194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([55, 55, 55, 24]), tensor([55, 53, 55, 55]), tensor([55, 55, 36, 55]), tensor([55, 55, 55, 55]), tensor([55, 55, 55, 55])]\n",
      "[tensor([55, 55, 24, 55]), tensor([53, 55, 55, 55]), tensor([55, 36, 55, 55]), tensor([55, 55, 55, 55]), tensor([55, 55, 55, 55])]\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    vocab, raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2f04eee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[55, 55, 55, 24],\n",
      "        [55, 53, 55, 55],\n",
      "        [55, 55, 36, 55],\n",
      "        [55, 55, 55, 55],\n",
      "        [55, 55, 55, 55],\n",
      "        [55, 55, 55, 55],\n",
      "        [55, 55, 55, 55],\n",
      "        [55, 51, 55, 55]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7cec55c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n",
      "tensor([[[-0.1819,  0.8828,  1.4373,  ...,  1.0165,  0.6598, -0.2191],\n",
      "         [-0.1819,  0.8828,  1.4373,  ...,  1.0165,  0.6598, -0.2191],\n",
      "         [-0.1819,  0.8828,  1.4373,  ...,  1.0165,  0.6598, -0.2191],\n",
      "         [ 1.0842,  1.5591, -0.5512,  ...,  1.0723, -0.2792,  0.5081]],\n",
      "\n",
      "        [[-0.1819,  0.8828,  1.4373,  ...,  1.0165,  0.6598, -0.2191],\n",
      "         [-1.1192,  0.5702, -0.4299,  ..., -0.9954,  0.1018, -1.1483],\n",
      "         [-0.1819,  0.8828,  1.4373,  ...,  1.0165,  0.6598, -0.2191],\n",
      "         [-0.1819,  0.8828,  1.4373,  ...,  1.0165,  0.6598, -0.2191]],\n",
      "\n",
      "        [[-0.1819,  0.8828,  1.4373,  ...,  1.0165,  0.6598, -0.2191],\n",
      "         [-0.1819,  0.8828,  1.4373,  ...,  1.0165,  0.6598, -0.2191],\n",
      "         [-0.8289,  0.8767,  1.2278,  ...,  0.0446, -0.8904,  0.2113],\n",
      "         [-0.1819,  0.8828,  1.4373,  ...,  1.0165,  0.6598, -0.2191]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1819,  0.8828,  1.4373,  ...,  1.0165,  0.6598, -0.2191],\n",
      "         [-0.1819,  0.8828,  1.4373,  ...,  1.0165,  0.6598, -0.2191],\n",
      "         [-0.1819,  0.8828,  1.4373,  ...,  1.0165,  0.6598, -0.2191],\n",
      "         [-0.1819,  0.8828,  1.4373,  ...,  1.0165,  0.6598, -0.2191]],\n",
      "\n",
      "        [[-0.1819,  0.8828,  1.4373,  ...,  1.0165,  0.6598, -0.2191],\n",
      "         [-0.1819,  0.8828,  1.4373,  ...,  1.0165,  0.6598, -0.2191],\n",
      "         [-0.1819,  0.8828,  1.4373,  ...,  1.0165,  0.6598, -0.2191],\n",
      "         [-0.1819,  0.8828,  1.4373,  ...,  1.0165,  0.6598, -0.2191]],\n",
      "\n",
      "        [[-0.1819,  0.8828,  1.4373,  ...,  1.0165,  0.6598, -0.2191],\n",
      "         [-2.0695, -0.4300, -0.8828,  ...,  1.1110, -0.5817,  0.9206],\n",
      "         [-0.1819,  0.8828,  1.4373,  ...,  1.0165,  0.6598, -0.2191],\n",
      "         [-0.1819,  0.8828,  1.4373,  ...,  1.0165,  0.6598, -0.2191]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)\n",
    "print(token_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0a9f1070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n",
      "Parameter containing:\n",
      "tensor([[-1.2656,  0.5118,  1.2998,  ...,  0.4382,  1.5726,  0.5510],\n",
      "        [-2.2960, -1.3788, -1.2873,  ...,  2.1713, -0.7664,  0.1969],\n",
      "        [-1.5482,  1.0006,  1.7814,  ...,  0.2665, -0.4404,  1.2006],\n",
      "        [-0.1979, -0.7609,  1.0254,  ..., -0.1898, -1.0215,  0.6873]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "print(pos_embedding_layer.weight.shape)\n",
    "\n",
    "# uncomment & execute the following line to see how the embedding layer weights look like\n",
    "print(pos_embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f773d281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n",
      "tensor([[ 0.2316, -1.5919, -2.6264,  ..., -1.0669,  1.0505, -0.0637],\n",
      "        [-1.0899,  0.0102, -1.5073,  ..., -0.2203, -0.0610,  1.1354],\n",
      "        [-1.9272,  1.1925, -0.3605,  ..., -1.3476, -1.2624, -2.2434],\n",
      "        [ 1.3647,  1.1420,  2.2042,  ..., -2.0222, -1.9595,  0.1530]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)\n",
    "\n",
    "# uncomment & execute the following line to see how the embeddings look like\n",
    "print(pos_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "83629a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n",
      "tensor([[[ 0.0497, -0.7091, -1.1891,  ..., -0.0504,  1.7103, -0.2828],\n",
      "         [-1.2718,  0.8930, -0.0700,  ...,  0.7962,  0.5988,  0.9163],\n",
      "         [-2.1091,  2.0753,  1.0768,  ..., -0.3310, -0.6026, -2.4625],\n",
      "         [ 2.4489,  2.7011,  1.6530,  ..., -0.9500, -2.2387,  0.6611]],\n",
      "\n",
      "        [[ 0.0497, -0.7091, -1.1891,  ..., -0.0504,  1.7103, -0.2828],\n",
      "         [-2.2090,  0.5805, -1.9372,  ..., -1.2157,  0.0408, -0.0129],\n",
      "         [-2.1091,  2.0753,  1.0768,  ..., -0.3310, -0.6026, -2.4625],\n",
      "         [ 1.1828,  2.0248,  3.6415,  ..., -1.0057, -1.2997, -0.0661]],\n",
      "\n",
      "        [[ 0.0497, -0.7091, -1.1891,  ..., -0.0504,  1.7103, -0.2828],\n",
      "         [-1.2718,  0.8930, -0.0700,  ...,  0.7962,  0.5988,  0.9163],\n",
      "         [-2.7561,  2.0693,  0.8673,  ..., -1.3029, -2.1527, -2.0321],\n",
      "         [ 1.1828,  2.0248,  3.6415,  ..., -1.0057, -1.2997, -0.0661]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0497, -0.7091, -1.1891,  ..., -0.0504,  1.7103, -0.2828],\n",
      "         [-1.2718,  0.8930, -0.0700,  ...,  0.7962,  0.5988,  0.9163],\n",
      "         [-2.1091,  2.0753,  1.0768,  ..., -0.3310, -0.6026, -2.4625],\n",
      "         [ 1.1828,  2.0248,  3.6415,  ..., -1.0057, -1.2997, -0.0661]],\n",
      "\n",
      "        [[ 0.0497, -0.7091, -1.1891,  ..., -0.0504,  1.7103, -0.2828],\n",
      "         [-1.2718,  0.8930, -0.0700,  ...,  0.7962,  0.5988,  0.9163],\n",
      "         [-2.1091,  2.0753,  1.0768,  ..., -0.3310, -0.6026, -2.4625],\n",
      "         [ 1.1828,  2.0248,  3.6415,  ..., -1.0057, -1.2997, -0.0661]],\n",
      "\n",
      "        [[ 0.0497, -0.7091, -1.1891,  ..., -0.0504,  1.7103, -0.2828],\n",
      "         [-3.1594, -0.4198, -2.3901,  ...,  0.8907, -0.6428,  2.0560],\n",
      "         [-2.1091,  2.0753,  1.0768,  ..., -0.3310, -0.6026, -2.4625],\n",
      "         [ 1.1828,  2.0248,  3.6415,  ..., -1.0057, -1.2997, -0.0661]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Combine token embeddings and positional embeddings\n",
    "# This is a common practice in LLMs to provide both the meaning of the token and its position in the sequence\n",
    "# The input embeddings are the sum of token embeddings and positional embeddings\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)\n",
    "\n",
    "# uncomment & execute the following line to see how the embeddings look like\n",
    "print(input_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
