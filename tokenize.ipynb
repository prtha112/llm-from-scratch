{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f437ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['เมื่อ', 'วัน', 'พฤหัสบดี', 'ที่', ' ', '31', ' ', 'ก.ค.', ' ', '2568', ' ', 'โดนัลด์', ' ', 'ทรัมป์', ' ', 'ประธานาธิบดี', 'สหรัฐฯ', ' ', 'ประกาศ', 'ผ่าน', ' ', 'Truth', ' ', 'Social', ' ', 'ว่า', ' ', 'เขา', 'จะ', 'เลื่อน', 'การบังคับใช้', 'มาตรการ', 'ภาษี', 'ต่าง', 'ตอบแทน', 'อัตรา', ' ', '30', '%', ' ', 'กับ', 'เม็กซิโก', 'ไป', 'ก่อน', ' ', 'โดย', 'จะ', 'ใช้', 'ข้อตกลง', 'เดิม', 'ซึ่ง', 'จะ', 'เก็บภาษี', 'สินค้า', 'บาง', 'รายการ', 'ที่', 'นำเข้า', 'จาก', 'เม็กซิโก', 'ใน', 'อัตรา', ' ', '25', '%', ' ', 'ต่อไป', 'อีก', ' ', '90', ' ', 'วัน', ' ', 'ในระหว่างนั้น', 'ทั้งสองฝ่าย', 'จะ', 'หาทาง', 'ทำ', 'ข้อตกลง', 'การค้า', 'ร่วมกัน']\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "from pythainlp.tokenize import word_tokenize\n",
    "\n",
    "text = \"เมื่อวันพฤหัสบดีที่ 31 ก.ค. 2568 โดนัลด์ ทรัมป์ ประธานาธิบดีสหรัฐฯ ประกาศผ่าน Truth Social ว่า เขาจะเลื่อนการบังคับใช้มาตรการภาษีต่างตอบแทนอัตรา 30% กับเม็กซิโกไปก่อน โดยจะใช้ข้อตกลงเดิมซึ่งจะเก็บภาษีสินค้าบางรายการที่นำเข้าจากเม็กซิโกในอัตรา 25% ต่อไปอีก 90 วัน ในระหว่างนั้นทั้งสองฝ่ายจะหาทางทำข้อตกลงการค้าร่วมกัน\"\n",
    "all_words = word_tokenize(text, engine=\"newmm\") \n",
    "print(all_words)\n",
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a0ee0ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'เมื่อ': 0,\n",
       " 'วัน': 71,\n",
       " 'พฤหัสบดี': 2,\n",
       " 'ที่': 56,\n",
       " ' ': 72,\n",
       " '31': 5,\n",
       " 'ก.ค.': 7,\n",
       " '2568': 9,\n",
       " 'โดนัลด์': 11,\n",
       " 'ทรัมป์': 13,\n",
       " 'ประธานาธิบดี': 15,\n",
       " 'สหรัฐฯ': 16,\n",
       " 'ประกาศ': 18,\n",
       " 'ผ่าน': 19,\n",
       " 'Truth': 21,\n",
       " 'Social': 23,\n",
       " 'ว่า': 25,\n",
       " 'เขา': 27,\n",
       " 'จะ': 75,\n",
       " 'เลื่อน': 29,\n",
       " 'การบังคับใช้': 30,\n",
       " 'มาตรการ': 31,\n",
       " 'ภาษี': 32,\n",
       " 'ต่าง': 33,\n",
       " 'ตอบแทน': 34,\n",
       " 'อัตรา': 61,\n",
       " '30': 37,\n",
       " '%': 64,\n",
       " 'กับ': 40,\n",
       " 'เม็กซิโก': 59,\n",
       " 'ไป': 42,\n",
       " 'ก่อน': 43,\n",
       " 'โดย': 45,\n",
       " 'ใช้': 47,\n",
       " 'ข้อตกลง': 78,\n",
       " 'เดิม': 49,\n",
       " 'ซึ่ง': 50,\n",
       " 'เก็บภาษี': 52,\n",
       " 'สินค้า': 53,\n",
       " 'บาง': 54,\n",
       " 'รายการ': 55,\n",
       " 'นำเข้า': 57,\n",
       " 'จาก': 58,\n",
       " 'ใน': 60,\n",
       " '25': 63,\n",
       " 'ต่อไป': 66,\n",
       " 'อีก': 67,\n",
       " '90': 69,\n",
       " 'ในระหว่างนั้น': 73,\n",
       " 'ทั้งสองฝ่าย': 74,\n",
       " 'หาทาง': 76,\n",
       " 'ทำ': 77,\n",
       " 'การค้า': 79,\n",
       " 'ร่วมกัน': 80}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddf57fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('เมื่อ', 0)\n",
      "('วัน', 71)\n",
      "('พฤหัสบดี', 2)\n",
      "('ที่', 56)\n",
      "(' ', 72)\n",
      "('31', 5)\n",
      "('ก.ค.', 7)\n",
      "('2568', 9)\n",
      "('โดนัลด์', 11)\n",
      "('ทรัมป์', 13)\n",
      "('ประธานาธิบดี', 15)\n",
      "('สหรัฐฯ', 16)\n",
      "('ประกาศ', 18)\n",
      "('ผ่าน', 19)\n",
      "('Truth', 21)\n",
      "('Social', 23)\n",
      "('ว่า', 25)\n",
      "('เขา', 27)\n",
      "('จะ', 75)\n",
      "('เลื่อน', 29)\n",
      "('การบังคับใช้', 30)\n",
      "('มาตรการ', 31)\n",
      "('ภาษี', 32)\n",
      "('ต่าง', 33)\n",
      "('ตอบแทน', 34)\n",
      "('อัตรา', 61)\n",
      "('30', 37)\n",
      "('%', 64)\n",
      "('กับ', 40)\n",
      "('เม็กซิโก', 59)\n",
      "('ไป', 42)\n",
      "('ก่อน', 43)\n",
      "('โดย', 45)\n",
      "('ใช้', 47)\n",
      "('ข้อตกลง', 78)\n",
      "('เดิม', 49)\n",
      "('ซึ่ง', 50)\n",
      "('เก็บภาษี', 52)\n",
      "('สินค้า', 53)\n",
      "('บาง', 54)\n",
      "('รายการ', 55)\n",
      "('นำเข้า', 57)\n",
      "('จาก', 58)\n",
      "('ใน', 60)\n",
      "('25', 63)\n",
      "('ต่อไป', 66)\n",
      "('อีก', 67)\n",
      "('90', 69)\n",
      "('ในระหว่างนั้น', 73)\n",
      "('ทั้งสองฝ่าย', 74)\n",
      "('หาทาง', 76)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9309bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = word_tokenize(text, engine=\"newmm\") \n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68c539da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 75, 29, 30, 31, 32, 33, 34, 61]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"เขาจะเลื่อนการบังคับใช้มาตรการภาษีต่างตอบแทนอัตรา\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98cf03e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'เขา จะ เลื่อน การบังคับใช้ มาตรการ ภาษี ต่าง ตอบแทน อัตรา'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c748335",
   "metadata": {},
   "source": [
    "Adding special context tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf2c6b41",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'--'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m tokenizer = SimpleTokenizerV1(vocab)\n\u001b[32m      3\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mเขาจะเลื่อนการบังคับใช้มาตรการ-- ภาษีต่างตอบแทนอัตรา\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m tokenizer.encode(text)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mSimpleTokenizerV1.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[32m      8\u001b[39m     preprocessed = word_tokenize(text, engine=\u001b[33m\"\u001b[39m\u001b[33mnewmm\u001b[39m\u001b[33m\"\u001b[39m) \n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     ids = [\u001b[38;5;28mself\u001b[39m.str_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[31mKeyError\u001b[39m: '--'"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"เขาจะเลื่อนการบังคับใช้มาตรการ-- ภาษีต่างตอบแทนอัตรา\"\n",
    "\n",
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dec10f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['เมื่อ', 'วัน', 'พฤหัสบดี', 'ที่', ' ']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "571b3db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(all_words)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {s:i for i,s in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a64373f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '%': 1,\n",
       " '25': 2,\n",
       " '2568': 3,\n",
       " '30': 4,\n",
       " '31': 5,\n",
       " '90': 6,\n",
       " 'Social': 7,\n",
       " 'Truth': 8,\n",
       " 'ก.ค.': 9,\n",
       " 'กับ': 10,\n",
       " 'การค้า': 11,\n",
       " 'การบังคับใช้': 12,\n",
       " 'ก่อน': 13,\n",
       " 'ข้อตกลง': 14,\n",
       " 'จะ': 15,\n",
       " 'จาก': 16,\n",
       " 'ซึ่ง': 17,\n",
       " 'ตอบแทน': 18,\n",
       " 'ต่อไป': 19,\n",
       " 'ต่าง': 20,\n",
       " 'ทรัมป์': 21,\n",
       " 'ทั้งสองฝ่าย': 22,\n",
       " 'ทำ': 23,\n",
       " 'ที่': 24,\n",
       " 'นำเข้า': 25,\n",
       " 'บาง': 26,\n",
       " 'ประกาศ': 27,\n",
       " 'ประธานาธิบดี': 28,\n",
       " 'ผ่าน': 29,\n",
       " 'พฤหัสบดี': 30,\n",
       " 'ภาษี': 31,\n",
       " 'มาตรการ': 32,\n",
       " 'รายการ': 33,\n",
       " 'ร่วมกัน': 34,\n",
       " 'วัน': 35,\n",
       " 'ว่า': 36,\n",
       " 'สหรัฐฯ': 37,\n",
       " 'สินค้า': 38,\n",
       " 'หาทาง': 39,\n",
       " 'อัตรา': 40,\n",
       " 'อีก': 41,\n",
       " 'เก็บภาษี': 42,\n",
       " 'เขา': 43,\n",
       " 'เดิม': 44,\n",
       " 'เมื่อ': 45,\n",
       " 'เม็กซิโก': 46,\n",
       " 'เลื่อน': 47,\n",
       " 'โดนัลด์': 48,\n",
       " 'โดย': 49,\n",
       " 'ใช้': 50,\n",
       " 'ใน': 51,\n",
       " 'ในระหว่างนั้น': 52,\n",
       " 'ไป': 53,\n",
       " '<|endoftext|>': 54,\n",
       " '<|unk|>': 55}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "518cf839",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = word_tokenize(text, engine=\"newmm\") \n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e81c712d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "มากเขาจะเลื่อนการบังคับใช้มาตรการภาษี <|endoftext|> ต่างตอบแทนอัตรา\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"มากเขาจะเลื่อนการบังคับใช้มาตรการภาษี\"\n",
    "text2 = \"ต่างตอบแทนอัตรา\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0134982e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[55, 43, 15, 47, 12, 32, 31, 54, 20, 18, 40]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ab74fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|> เขา จะ เลื่อน การบังคับใช้ มาตรการ ภาษี <|endoftext|> ต่าง ตอบแทน อัตรา'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc02b99",
   "metadata": {},
   "source": [
    "Data sampling with a sliding window\n",
    "We train LLMs to generate one word at a time, so we want to prepare the training data accordingly where the next word in a sequence represents the target to predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb7c90e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "543\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1dcc37b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0c4345c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [55, 55, 55, 55]\n",
      "y:      [55, 55, 55, 45]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e25e3ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<|unk|> <|unk|> <|unk|> <|unk|>', '<|unk|> <|unk|> <|unk|> เมื่อ')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(x), tokenizer.decode(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be11ae7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|> ----> <|unk|>\n",
      "<|unk|> <|unk|> ----> <|unk|>\n",
      "<|unk|> <|unk|> <|unk|> ----> <|unk|>\n",
      "<|unk|> <|unk|> <|unk|> <|unk|> ----> เมื่อ\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
